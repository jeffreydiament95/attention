{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**ATTENTION FROM SCRATCH**</u>\n",
    "\n",
    "This notebook provides a step by step walkthrough for how implement a single head of self-attention from the scratch using pytorch.\n",
    "\n",
    "inspired by Karpathy's transformer tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7\n",
    "\n",
    "*core problem*: how to gather information from the past in a data-dependent way?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**DEFINITIONS**</u>\n",
    "\n",
    "**B**: batch size (numner of examples in a training batch, processed in parallel)\n",
    "\n",
    "**T**: time (context length, number of tokens per batch example)\n",
    "\n",
    "**C**: channels (embedding dimension, number of elements used to represent each token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42) # for reproducibility\n",
    "B, T, C = 4, 8, 2 # batch size, time steps, channels\n",
    "x = torch.rand(B, T, C) # input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERSION 1**\n",
    "\n",
    "<u>Objective</u>: create random input **x**, and then calculate **out**, which is an identically sized tensor containing the average of all the proceeding tokens for a given exmample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do computation\n",
    "\n",
    "xbow1 = torch.zeros(B, T, C) # pre-allocate output\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1, ] # all previous time steps (t, C)\n",
    "        xprev_mean = torch.mean(xprev, dim=0) # average over time (C)\n",
    "        xbow1[b, t] = xprev_mean # average over time (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([4, 8, 2])\n",
      "xbow1.shape = torch.Size([4, 8, 2])\n",
      "x[0,:3] = tensor([[0.8823, 0.9150],\n",
      "        [0.3829, 0.9593],\n",
      "        [0.3904, 0.6009]])\n",
      "xbow1[0,:3] = tensor([[0.8823, 0.9150],\n",
      "        [0.6326, 0.9372],\n",
      "        [0.5519, 0.8251]])\n"
     ]
    }
   ],
   "source": [
    "# evaluate output\n",
    "\n",
    "print(f'{x.shape = }')\n",
    "print(f'{xbow1.shape = }')\n",
    "\n",
    "print(f'{x[0,:3] = }')\n",
    "print(f'{xbow1[0,:3] = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Evaluation</u>: as shown above, for the first 3 timesteps of the first batch, the output matches our objective. For the first token, both output channels match the input channels (mean over one sample). For the next token, the output channel is the mean of the first two input channels, etc...\n",
    "\n",
    "For the next two versions, we'll verify the results by comparing against this output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERSION 2**\n",
    "\n",
    "<u>Objective</u>: Same as above, but do it more efficiently using matrix multiplication. This allows us to do an average aggregation of past information (no future) using a lower triangular matrix multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# do computation \n",
    "\n",
    "wei = torch.tril(torch.ones(T, T)) # lower triangular matrix (T, T)\n",
    "\n",
    "# dim=1 means \"reduce over column dimension\"\n",
    "# keepdim=True is critical, because it maintains the empty column dimension which is needed for broadcasting (see tangent below)\n",
    "wei = wei / wei.sum(dim=1, keepdim=True) # normalize (T, T)\n",
    "xbow2 = wei @ x # matrix multiplication (T, T) @ (B, T, C) -> (B, T, C); broadcast (T, T) over batch dimension B and then do (T, T) @ (T, C) for each batch in parallel\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate output\n",
    "\n",
    "torch.allclose(xbow1, xbow2) # check that outputs are equal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERSION 3**\n",
    "\n",
    "<u>Objective</u>: Same as above, but introduce softmax to now allow for *weighted* aggregation of past information. Here we're still doing an average, but introducing softmax allows us to handle non uniform weight distributions in the future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before softmax: \n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "after softmax: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# do computation \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular matrix (T, T)\n",
    "wei = torch.zeros(T, T) # weight matrix (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # mask out upper triangular part (T, T)\n",
    "print(\"before softmax: \\n\", wei)\n",
    "wei = torch.softmax(wei, dim=1) # softmax over column dimension (T, T)\n",
    "print(\"after softmax: \\n\", wei)\n",
    "xbow3 = wei @ x # matrix multiplication (T, T) @ (B, T, C) -> (B, T, C); broadcast (T, T) over batch dimension B and then do (T, T) @ (T, C) for each batch in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate output\n",
    "\n",
    "torch.allclose(xbow1, xbow3) # check that outputs are equal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**VERSION 4: SELF ATTENTION**</u>\n",
    "\n",
    "<u>Objective</u>: Implement a single head of self attention. Here, each token emits three vectors of length *head_size*:\n",
    "\n",
    "1. query: \"what am I looking for?\"\n",
    "\n",
    "2. key: \"what I contain\"\n",
    "\n",
    "3. value: \"what I will communicate\"\n",
    "\n",
    "weight matrix contains the affinities for each token with respect to all other tokens in the example. for a given token_1, it's affinity to any other token_x in the example is calculated via a dot product between token_1's query and token_x's key. we scale this up with matrix multiplication to compute the affinities between all tokens for an example and across batches in parallel. the weight matrix is then normalized and future info is omitted via masking and softmax.\n",
    "\n",
    "output is then determined via matrix multiplication between the weight matrix and the value matrix. \n",
    "\n",
    "we can think of the input *x* as containing private information. it only communicates outwardly via query, key, and value, which are all calculated via a linear layer that transforms each token in x from *C* -> *head_size*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "# linear layers used to project *private* input into *public* key, query, and value vectors\n",
    "query = nn.Linear(C, head_size, bias=False) # converts (B, T, C) -> (B, T, head_size)\n",
    "key = nn.Linear(C, head_size, bias=False) # converts (B, T, C) -> (B, T, head_size)\n",
    "value = nn.Linear(C, head_size, bias=False) # converts (B, T, C) -> (B, T, head_size)\n",
    "\n",
    "q = query(x) # (B, T, head_size) \n",
    "k = key(x) # (B, T, head_size) NOTE: here we implement *self* attention bc k is derived from the input; but in *cross-attention*, this would come from elsewhere (the encoder, not this input)\n",
    "v = value(x) # (B, T, head_size) NOTE: here we implement *self* attention bc v is derived from the input; but in *cross-attention*, this would come from elsewhere (the encoder, not this input)\n",
    "\n",
    "wei = q @ k.transpose(-1, -2) * head_size**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "# mask to eliminate future info and normalize via softmax\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular matrix (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # mask out upper triangular part (B, T, T), broadcast over batch dimension B NOTE: this implements a *decoder* bc we ignore the future; if we wanted to implement an *encoder*, we would not mask out any of this info\n",
    "wei = F.softmax(wei, dim=-1) # softmax over last dimension (T, T)\n",
    "\n",
    "out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TANGENT: IMPORTANCE OF SCALED ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no scaling: \n",
      " tensor(0.9493) tensor(1.0839) tensor(15.8095)\n",
      "with scaling: \n",
      " tensor(0.9493) tensor(1.0839) tensor(0.9881)\n"
     ]
    }
   ],
   "source": [
    "# assume that k and q have unit variance at initialization -- observe how this variance translates to the variance of the weights\n",
    "\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei_noscale = q @ k.transpose(-1, -2)\n",
    "wei_scale = q @ k.transpose(-1, -2) * head_size**-0.5\n",
    "\n",
    "print(\"no scaling: \\n\", k.var(), q.var(), wei_noscale.var())\n",
    "print(\"with scaling: \\n\", k.var(), q.var(), wei_scale.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'softmax output trends towards onehot vector as variance of input increases')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# high variance of weights prior to softmax is problematic, as illustrated below\n",
    "\n",
    "# loop through scaling factors from 1 to 100 in steps of 10 and observe how the variance of the weights changes and how this effects the shape of the softmax distribution\n",
    "torch.manual_seed(42) # for reproducibility\n",
    "\n",
    "x = torch.randn(10,) \n",
    "# create range of integers from 1 to 10 with step size of 2\n",
    "scale = list(range(1,6,1))[::-1]\n",
    "x_scaled = torch.stack([x * i for i in scale])\n",
    "vars = [var.item() for var in x_scaled.var(dim=1)]\n",
    "x_softmaxes = F.softmax(x_scaled, dim=1)\n",
    "\n",
    "# plot softmax distributions \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(x_softmaxes.numpy(),vmin=0,vmax=1, cmap=\"jet\")\n",
    "ax.set_yticks(range(len(vars)))\n",
    "ax.set_yticklabels([\"{:.2f}\".format(var) for var in vars])\n",
    "ax.set_xlabel('input vector')\n",
    "ax.set_ylabel('input variance')\n",
    "ax.set_title(\"softmax output trends towards onehot vector as variance of input increases\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TANGENT: BROADCASTING AND THE IMPORANCE OF KEEPDIM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_incorrect.shape = torch.Size([8])\n",
      "sum_correct.shape = torch.Size([8, 1])\n",
      "sum_incorrect = tensor([1., 2., 3., 4., 5., 6., 7., 8.])\n",
      "sum_correct = tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.]])\n"
     ]
    }
   ],
   "source": [
    "# tangent: importance of using the correct setting for keepdim\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T)) # lower triangular matrix (T, T)\n",
    "\n",
    "sum_incorrect = wei.sum(1) # keepdim=False by default; output is row vector (T)\n",
    "sum_correct = wei.sum(1, keepdim=True) # output is column vector (T, 1)\n",
    "\n",
    "print(f'{sum_incorrect.shape = }')\n",
    "print(f'{sum_correct.shape = }')\n",
    "\n",
    "print(f'{sum_incorrect = }')\n",
    "print(f'{sum_correct = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "during wei = wei / sum, the following will happen:\n",
    "\n",
    "*for sum_incorrect:*\n",
    "\n",
    "(T, T) / (T)\n",
    "\n",
    "(T, T) / ( , T) shift to the right\n",
    "\n",
    "(T, T) / (T, T) duplicate *row* vector down all the columns -- WRONG!\n",
    "\n",
    "\n",
    "*for sum_correct:*\n",
    "\n",
    "(T, T) / (T, 1) \n",
    "\n",
    "(T, T) / (T, T) duplicate *column* vector across all the rows -- CORRECT!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
